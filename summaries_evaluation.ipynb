{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNTxh359q0rItvmPSh+USYP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreetishresthanp/medical_summaries_evaluation/blob/main/summaries_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Set up Environment and Install Dependencies"
      ],
      "metadata": {
        "id": "QPkW95Ww3M-o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-VNkNlOGcOf"
      },
      "outputs": [],
      "source": [
        "# Verify GPU setup\n",
        "! nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformers installation\n",
        "! pip install transformers[torch] datasets evaluate rouge_score\n",
        "# Install dependencies\n",
        "! pip install torch\n",
        "! pip install bert_score\n",
        "! pip install textstat"
      ],
      "metadata": {
        "id": "WvPePDqJG6HK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optional huggingface authentication using token\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "aiVFIedp369E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load dataset"
      ],
      "metadata": {
        "id": "GzGsypPR4Ate"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# load test_data.csv\n",
        "med_dataset = load_dataset(\"csv\", data_files=\"test_data.csv\", split = \"train\")\n",
        "med_dataset"
      ],
      "metadata": {
        "id": "Eoxb8CPQHNtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of a record in the dataset\n",
        "med_dataset[0]"
      ],
      "metadata": {
        "id": "Fu3KjlkThj9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generate Summaries"
      ],
      "metadata": {
        "id": "deId93MxExlX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # checks if gpu is available\n",
        "pipeline_device = 0 if device == \"cuda\" else -1 # for determining if we want to load model in GPU or CPU"
      ],
      "metadata": {
        "id": "qzA-sTC7vTDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"Falconsai/medical_summarization\"\n",
        "prompt = \"Generate a plain language summary that is easy to read highlighting key points and removing unnecessary details that can be easily understood by non-medical people : \""
      ],
      "metadata": {
        "id": "Uiq_OhsMi_cG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "model = AutoModel.from_pretrained(model_id)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "k3UHSqpwvcPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Process inputs in batches to avoid running out of memory in Colab when testing larger models\n",
        "batch_size = 16 # Adjust this value based on available memory\n",
        "summarized_outputs = []\n",
        "summarizer = pipeline(\"summarization\", model=model_id, tokenizer=tokenizer, device=pipeline_device)\n",
        "\n",
        "for i in range(0, len(med_dataset[\"abstract_text\"]), batch_size):\n",
        "    inputs_batch = [prompt + doc for doc in med_dataset[\"abstract_text\"][i:i + batch_size]]\n",
        "    outputs_batch = summarizer(inputs_batch, min_length=20, max_length=150, do_sample=False)\n",
        "    summarized_outputs.extend([output[\"summary_text\"] for output in outputs_batch])"
      ],
      "metadata": {
        "id": "iggQuIaPCkc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Write outputs to file"
      ],
      "metadata": {
        "id": "ggb481MIFGMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "with open(\"summarized_outputs.csv\", \"w\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"abstract_text\", \"target_text\", \"generated_text\"])\n",
        "    for i in range(len(summarized_outputs)):\n",
        "      writer.writerow([med_dataset[\"abstract_text\"][i], med_dataset[\"target_text\"][i], summarized_outputs[i]])"
      ],
      "metadata": {
        "id": "12A0uqg0C8pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "D-UW-vYoFYnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "import textstat"
      ],
      "metadata": {
        "id": "UN-sgJTv5-Ya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = evaluate.load(\"rouge\")\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "bertscore = evaluate.load(\"bertscore\")"
      ],
      "metadata": {
        "id": "jx_iRRZz-2_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ROUGE Score"
      ],
      "metadata": {
        "id": "b8rVY5amLzDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agg_rouge_scores = rouge.compute(predictions=summarized_outputs, references=med_dataset[\"target_text\"], use_stemmer=True, use_aggregator=True)\n",
        "agg_rouge_scores"
      ],
      "metadata": {
        "id": "CA6pI1uqHJvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statistics import mean\n",
        "print(\"Average of Rouge Scores (ROUGE-1, ROUGE-2, and ROUGE-L): \", mean([agg_rouge_scores['rouge1'], agg_rouge_scores['rouge2'], agg_rouge_scores['rougeL']]))"
      ],
      "metadata": {
        "id": "e-Ij6YpMHQz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "rouge_scores = rouge.compute(predictions=summarized_outputs, references=med_dataset[\"target_text\"], use_stemmer=True, use_aggregator=False)\n",
        "metric_df = pd.DataFrame(rouge_scores)\n",
        "metric_df.drop(columns=[\"rougeLsum\"], inplace=True)\n",
        "metric_df[\"avg_rouge_score\"] = metric_df.mean(axis=1)\n",
        "metric_df.head()\n"
      ],
      "metadata": {
        "id": "5hOkoxD0CIyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Box plot for avg rouge scores of all records\n",
        "import matplotlib.pyplot as plt\n",
        "metric_df.boxplot(column='avg_rouge_score')\n",
        "plt.title('Boxplot of Avg Rouge Score (ROUGE-1, ROUGE-2, and ROUGE-L)')\n",
        "plt.ylabel('Avg Rouge Score (0 - 1)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fV1XE1b4LMcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.scatterplot(data=metric_df, y=\"avg_rouge_score\", x = range(len(metric_df)))"
      ],
      "metadata": {
        "id": "jZWQQtC_Itsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### BLEU Score"
      ],
      "metadata": {
        "id": "IEZ0CRwIL2kB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_score = bleu.compute(predictions=summarized_outputs, references=med_dataset[\"target_text\"])\n",
        "bleu_score"
      ],
      "metadata": {
        "id": "jb-vlNIK_PrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"BLEU Score: {bleu_score['bleu'] * 100:.2f}\")"
      ],
      "metadata": {
        "id": "LHAUIXTtNzL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### BERT Score"
      ],
      "metadata": {
        "id": "2OjbOHYAN_P0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_score = bertscore.compute(predictions=summarized_outputs, references=med_dataset[\"target_text\"], lang=\"en\")\n",
        "bert_score"
      ],
      "metadata": {
        "id": "lPifbXUBNrqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_metric_df = pd.DataFrame(bert_score)\n",
        "bert_metric_df.drop(columns=[\"hashcode\"], inplace=True)\n",
        "bert_metric_df[\"precision\"] = (bert_metric_df[\"precision\"]*100).round(2)\n",
        "bert_metric_df[\"recall\"] = (bert_metric_df[\"recall\"]*100).round(2)\n",
        "bert_metric_df[\"f1\"] = (bert_metric_df[\"f1\"]*100).round(2)\n",
        "bert_metric_df.head()"
      ],
      "metadata": {
        "id": "2RwkIQVVOgye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision_mean = bert_metric_df['precision'].mean()\n",
        "recall_mean = bert_metric_df['recall'].mean()\n",
        "f1_mean = bert_metric_df['f1'].mean()\n",
        "print(f\"Precision Mean: {precision_mean:.2f}\")\n",
        "print(f\"Recall Mean: {recall_mean:.2f}\")\n",
        "print(f\"F1 Mean: {f1_mean:.2f}\")"
      ],
      "metadata": {
        "id": "GtClWbgLGNmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Box plot for precision scores of all records\n",
        "bert_metric_df.boxplot(column='precision')\n",
        "plt.title('Boxplot of Precision Score (BERT)')\n",
        "plt.ylabel('Score (%)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hc40Lv11PSiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Box plot for precision scores of all records\n",
        "bert_metric_df.boxplot(column='recall')\n",
        "plt.title('Boxplot of Recall (BERT)')\n",
        "plt.ylabel('Score (%)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MxwLojWkPgyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Box plot for precision scores of all records\n",
        "bert_metric_df.boxplot(column='f1')\n",
        "plt.title('Boxplot of F1 Accuracy (BERT)')\n",
        "plt.ylabel('Score (%)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Uo7hS5LEPnQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del bert_score['hashcode']"
      ],
      "metadata": {
        "id": "0Ph3LQyaUZy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.boxplot(bert_score.values(), labels=bert_score.keys())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_0RurkCqSna2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  Flesch-Kincaid Grade Level (Readability Metric)"
      ],
      "metadata": {
        "id": "xy7wozV8QuEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flesch_kincaid_grades = [textstat.flesch_kincaid_grade(summary) for summary in summarized_outputs]\n",
        "flesch_reading_ease = [textstat.flesch_reading_ease(summary) for summary in summarized_outputs]\n",
        "\n",
        "readability_scores = {\n",
        "    \"flesch_kincaid_grade\": flesch_kincaid_grades,\n",
        "    \"flesch_reading_ease\": flesch_reading_ease\n",
        "}\n",
        "# flesch_kincaid_grades[:5]\n",
        "# flesch_reading_ease[:5]"
      ],
      "metadata": {
        "id": "FXKdDD2HRHjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statistics import mean\n",
        "print(\"Average of Flesch-Kincaid Grade Level (FKGL): \", mean(flesch_kincaid_grades))\n",
        "print(\"Average of Flesch Reading Ease (FRE): \", mean(flesch_reading_ease))"
      ],
      "metadata": {
        "id": "P22BU88uG6pZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.boxplot(readability_scores.values(), labels=readability_scores.keys())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bx3WJLZ8U5br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Human Evaluation Metrics"
      ],
      "metadata": {
        "id": "fN9xA1YOYVG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_1 = [2,2,3,3,3,4,1,1,2,2,3,4,2,3,3,2,3,4,2,3,3,2,3,4,4,4,4,3,3,3]\n",
        "q_2 = [1,2,3,3,3,3,1,1,2,1,2,3,1,2,2,1,3,3,1,3,4,1,3,3,3,4,4,2,3,3]\n",
        "q_3 = [2,2,2,1,2,3,2,2,3,2,2,3,2,2,3,2,2,3,2,2,3,1,2,4,3,3,3,2,3,3]\n",
        "q_4 = [1,2,3,3,3,4,1,1,2,1,3,3,1,2,2,2,3,4,1,2,2,2,2,3,2,3,3,2,3,3]\n",
        "q_5 = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,1,1,2]"
      ],
      "metadata": {
        "id": "lX7fxUmISoRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = {'q1': q_1, 'q2': q_2, 'q3': q_3, 'q4': q_4, 'q5': q_5}\n",
        "df = pd.DataFrame(data=d)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "kHAz46f5VesP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.boxplot(df.values, labels=df.keys())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gl7Ux6WfV7qU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_q1 = df['q1'].mean()\n",
        "mean_q2 = df['q2'].mean()\n",
        "mean_q3 = df['q3'].mean()\n",
        "mean_q4 = df['q4'].mean()\n",
        "mean_q5 = df['q5'].mean()\n",
        "print(\"Average of Question 1: \", mean_q1)\n",
        "print(\"Average of Question 2: \", mean_q2)\n",
        "print(\"Average of Question 3: \", mean_q3)\n",
        "print(\"Average of Question 4: \", mean_q4)\n",
        "print(\"Average of Question 5: \", mean_q5)"
      ],
      "metadata": {
        "id": "fyeG8tNMVz7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "std_1 = df['q1'].std()\n",
        "std_2 = df['q2'].std()\n",
        "std_3 = df['q3'].std()\n",
        "std_4 = df['q4'].std()\n",
        "std_5 = df['q5'].std()\n",
        "print(\"Standard Deviation of Question 1: \", std_1)\n",
        "print(\"Standard Deviation of Question 2: \", std_2)\n",
        "print(\"Standard Deviation of Question 3: \", std_3)\n",
        "print(\"Standard Deviation of Question 4: \", std_4)\n",
        "print(\"Standard Deviation of Question 5: \", std_5)"
      ],
      "metadata": {
        "id": "W3e1II1PWkMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statistics import median\n",
        "median_q1 = df['q1'].median()\n",
        "median_q2 = df['q2'].median()\n",
        "median_q3 = df['q3'].median()\n",
        "median_q4 = df['q4'].median()\n",
        "median_q5 = df['q5'].median()\n",
        "print(\"Median of Question 1: \", median_q1)\n",
        "print(\"Median of Question 2: \", median_q2)\n",
        "print(\"Median of Question 3: \", median_q3)\n",
        "print(\"Median of Question 4: \", median_q4)\n",
        "print(\"Median of Question 5: \", median_q5)"
      ],
      "metadata": {
        "id": "3IVKiBY031ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "iqr_q1 = stats.iqr(df['q1'])\n",
        "iqr_q2 = stats.iqr(df['q2'])\n",
        "iqr_q3 = stats.iqr(df['q3'])\n",
        "iqr_q4 = stats.iqr(df['q4'])\n",
        "iqr_q5 = stats.iqr(df['q5'])\n",
        "print(\"IQR of Question 1: \", iqr_q1)\n",
        "print(\"IQR of Question 2: \", iqr_q2)\n",
        "print(\"IQR of Question 3: \", iqr_q3)\n",
        "print(\"IQR of Question 4: \", iqr_q4)\n",
        "print(\"IQR of Question 5: \", iqr_q5)"
      ],
      "metadata": {
        "id": "P7shFxsD4a5V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}